# -*- coding: utf-8 -*-
"""CrimeDataAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y-8OT5gO1FGvFGhSg9ELqyTD3eeRqEz5

# Crime Trend and Crime Type Prediction for Chicago and New York

## Library Import and Environment Setup
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score
from sklearn import metrics

from google.colab import drive
drive.mount('/content/drive/')

"""## Data Import and Cleaning

### New York Dataset

#### New York historic dataset
"""

NY_df_historic = pd.read_csv('/content/drive/MyDrive/NYPD_Arrests_Data__Historic__20240310.csv')

NY_df_historic.shape

print(NY_df_historic['ARREST_KEY'].isna().sum())
print(NY_df_historic['ARREST_DATE'].isna().sum())
print(NY_df_historic['KY_CD'].isna().sum()) # Three digit internal classification code (more general category than PD code)
print(NY_df_historic['OFNS_DESC'].isna().sum()) # Description of internal classification corresponding with KY code (more general category than PD description)

NY_df_historic = NY_df_historic[NY_df_historic['KY_CD'].notna()]
NY_df_historic = NY_df_historic[NY_df_historic['OFNS_DESC'].notna()]

print(NY_df_historic['ARREST_KEY'].isna().sum())
print(NY_df_historic['ARREST_DATE'].isna().sum())
print(NY_df_historic['KY_CD'].isna().sum()) # Three digit internal classification code (more general category than PD code)
print(NY_df_historic['OFNS_DESC'].isna().sum()) # Description of internal classification corresponding with KY code (more general category than PD description)

print(NY_df_historic['ARREST_DATE'].max())
print(NY_df_historic['ARREST_DATE'].min())

NY_df_historic.shape

"""#### New York dataset of 2023"""

NY_df = pd.read_csv('/content/drive/MyDrive/NYPD_Arrest_Data__Year_to_Date__20240228.csv')

NY_df.shape

NY_df.columns

print(NY_df['ARREST_KEY'].isna().sum())
print(NY_df['ARREST_DATE'].isna().sum())
print(NY_df['KY_CD'].isna().sum()) # Three digit internal classification code (more general category than PD code)
print(NY_df['OFNS_DESC'].isna().sum()) # Description of internal classification corresponding with KY code (more general category than PD description)

NY_df = NY_df[NY_df['KY_CD'].notna()]

NY_df.shape

"""Since we found there are 17 NA values in the column KY_CD in New York dataset, we removed those rows to prepare the dataset for later analysis and predictions."""

print(NY_df['ARREST_DATE'].max())
print(NY_df['ARREST_DATE'].min())

"""### Chicago Dataset"""

# Chicago_df_all is a complete Chicago dataset
Chicago_df_all = pd.read_csv('/content/drive/MyDrive/Crimes_-_2001_to_Present_20240303.csv')

Chicago_df_all.shape

Chicago_df_all.columns

print(Chicago_df_all['ID'].isna().sum())
print(Chicago_df_all['Case Number'].isna().sum())
print(Chicago_df_all['Date'].isna().sum())
print(Chicago_df_all['IUCR'].isna().sum()) # Illinois Unifrom Crime Reporting code, directly linked to the Primary Type and Description.
print(Chicago_df_all['District'].isna().sum())

Chicago_df_all = Chicago_df_all[Chicago_df_all['District'].notna()]

print(Chicago_df_all['District'].isna().sum())

"""We found all the records in Chicago dataset contain values for ID, Case Number,Date, and IUCR but there are some NA values for column District. Hence we removed those records to prepare our dataset for the analysis and prediction models."""

print(Chicago_df_all['Date'].max())
print(Chicago_df_all['Date'].min())

"""#### Chicago dataset of 2023
To compare results of analyzing Chicago and New York crime, we extract one seperate dataset for Chicago which has date ranging from 01/01/2023 to 12/31/2023 like the New York dataset of 2023.
"""

Chicago_df = Chicago_df_all[Chicago_df_all['Date'] >= '01/01/2023']

print(Chicago_df['Date'].max())
print(Chicago_df['Date'].min())

Chicago_df.shape

"""## Model 1

### Chicago

#### Visulization of relationship between the number of arrests and crime type
"""

# filter out the incidents when arrests did not happen
Chicago_arrest_df = Chicago_df_all[Chicago_df_all['Arrest']]

# Aggregate Chicago dataset based on Primary Crime Type
ch_vis = Chicago_arrest_df.groupby(['Primary Type']).count().reset_index()

len(Chicago_df_all['Primary Type'].unique())

ch_vis = ch_vis.iloc[:, 0:2]
ch_vis = ch_vis.set_axis(['Type', 'Count'], axis=1)
ch_vis.sort_values(by=['Count'], ascending=False)

# Aggregate Chicago dataset based on Primary Crime Type and District
Chicago_arrest_df['District'] = Chicago_arrest_df['District'].astype(str)
ch_vis = Chicago_arrest_df.groupby(['Primary Type', 'District']).count().reset_index()

ch_vis = ch_vis.iloc[:, 0:3]
ch_vis = ch_vis.set_axis(['Type', 'District', 'Count'], axis=1)
ch_vis.sort_values(by=['Count'], ascending=False).head(20)

fig = plt.figure(figsize = (10, 5))
plt.bar(ch_vis['Type'], ch_vis['Count'], color='maroon', width = 0.4)
plt.xlabel("Primary Cirme Type")
plt.ylabel("Number of Arrests")
plt.title("Number of Arrests for Types of Crime")
plt.xticks(rotation=45, ha='right')
plt.show()

"""#### Logistic Regression: Find out how the district where the incident happened and the crime type affect the occurence of the arrest"""

stacked_barchart_df = Chicago_df_all.groupby(['Arrest', 'Primary Type']).size().unstack()
stacked_barchart_df.apply(lambda x:x/x.sum(), axis=1).plot(kind='barh', stacked=True)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

Chicago_df_historic = Chicago_df_all[Chicago_df_all['Date'] < '01/01/2023 01:00:00 AM']

print(Chicago_df_historic['Date'].max())
print(Chicago_df_historic['Date'].min())

len(Chicago_df_historic['Primary Type'].unique())

len(Chicago_df['Primary Type'].unique())

# Prepare for model creation: filter out the crime types that exist in Chicago dataset but not in historic Chicago dataset
Chicago_df = Chicago_df[Chicago_df['Primary Type'].isin(Chicago_df_historic['Primary Type'])]

len(Chicago_df_historic['Primary Type'].unique())

len(Chicago_df['Primary Type'].unique())

# Chicago historic dataset is used as training data
X_train = Chicago_df_historic[['District', 'Primary Type']]
X_train = pd.get_dummies(data=X_train, drop_first=True)
y_train = (Chicago_df_historic['Arrest']).astype(int)
# converted column label to int type, 1 stands for True and 0 stands for False

# Chicago dataset of 2023 is used as testing data
X_test = Chicago_df[['District', 'Primary Type']]
X_test = pd.get_dummies(data=X_test, drop_first=True)
y_test = (Chicago_df['Arrest']).astype(int)

m = LogisticRegression()
m.fit(X_train, y_train)

y_hat = m.predict(X_test)

accuracy_score(y_test, y_hat)

confusion_matrix(y_test, y_hat)

fpr, tpr, thresholds = roc_curve(y_test, y_hat)
roc_auc = roc_auc_score(y_test, y_hat)

plt.plot(fpr, tpr, label='ROC Curve (area = %0.3f)' %roc_auc)
plt.title('ROC curve (area = %0.3f)' %roc_auc)
plt.xlabel('FP rate')
plt.ylabel('TP rate')

"""### New York

#### Visulization of relationship between the number of arrests and crime type
"""

# Aggregate New York dataset based on crime description
ny_vis = NY_df.groupby(['OFNS_DESC']).count().reset_index()

ny_vis = ny_vis.iloc[:, 0:2]
ny_vis = ny_vis.set_axis(['Type', 'Count'], axis=1)
ny_vis.sort_values(by=['Count'], ascending=False).head(20)

fig = plt.figure(figsize = (17, 5))
plt.bar(ny_vis['Type'], ny_vis['Count'], color='maroon', width = 0.4)
plt.xlabel("Primary Cirme Type")
plt.ylabel("Number of Arrest")
plt.title("Number of Arrest for Types of Crime")
plt.xticks(rotation=45, ha='right')
plt.show()

"""#### Linear Regression: Find out the relationship between the number of arrest and crime description and arrest borough"""

len(NY_df_historic['OFNS_DESC'].unique())

len(NY_df['OFNS_DESC'].unique())

NY_df_historic = NY_df_historic[NY_df_historic['OFNS_DESC'].isin(NY_df['OFNS_DESC'])]

len(NY_df_historic['OFNS_DESC'].unique())

len(NY_df['OFNS_DESC'].unique())

NY_df_historic['ARREST_BORO'].unique()

NY_df['ARREST_BORO'].unique()

NY_df_historic = NY_df_historic[NY_df_historic['ARREST_BORO'].notna()]

NY_df_historic['ARREST_BORO'].unique()

# Aggregate New York dataset based on crime description and borough
ny_historic_aggre = NY_df_historic.groupby(['OFNS_DESC', 'ARREST_BORO']).count().reset_index().iloc[:, 0:3]
ny_2023_aggre = NY_df.groupby(['OFNS_DESC', 'ARREST_BORO']).count().reset_index().iloc[:, 0:3]

ny_historic_aggre = ny_historic_aggre.set_axis(['Type', 'Borough', 'Count'], axis=1)
ny_2023_aggre = ny_2023_aggre.set_axis(['Type', 'Borough', 'Count'], axis=1)

ny_2023_aggre

X_train = pd.get_dummies(data=ny_historic_aggre[['Type', 'Borough']], drop_first=True)
y_train = ny_historic_aggre['Count']

X_test = pd.get_dummies(data=ny_2023_aggre[['Type', 'Borough']], drop_first=True)
y_test = ny_2023_aggre['Count']

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# print the intercept
print(model.intercept_)

# print the coefficients
coeff_parameter = pd.DataFrame(model.coef_, X_train.columns, columns=['Coefficient'])
coeff_parameter.sort_values(by=['Coefficient'])

y_hat = model.predict(X_test)

print('Mean:', metrics.mean_absolute_error(y_test, y_hat))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_hat))
print('Squared Root of Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_hat)))

"""## Model 2: Random Forest and XGBoost"""

from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

def drop_missing_columns(df):
    return df.dropna(axis=1)

# Aggregate Chicago dataset based on Primary Crime Type
aggregated_df_chicago = Chicago_arrest_df.groupby(['Primary Type']).count().reset_index()

aggregated_df_chicago = aggregated_df_chicago.iloc[:, 0:2]
aggregated_df_chicago = aggregated_df_chicago.set_axis(['Type', 'Count'], axis=1)

# Aggregate New York dataset based on crime description
aggregated_df_ny = NY_df.groupby(['OFNS_DESC']).count().reset_index()

aggregated_df_ny = aggregated_df_ny.iloc[:, 0:2]
aggregated_df_ny = aggregated_df_ny.set_axis(['Type', 'Count'], axis=1)

X_chicago = aggregated_df_chicago.drop(columns=['Count'])
y_chicago = aggregated_df_chicago['Count']

X_ny = aggregated_df_ny.drop(columns=['Count'])
y_ny = aggregated_df_ny['Count']

X_train_chicago, X_test_chicago, y_train_chicago, y_test_chicago = train_test_split(X_chicago, y_chicago, test_size=0.3, random_state=42)
X_train_ny, X_test_ny, y_train_ny, y_test_ny = train_test_split(X_ny, y_ny, test_size=0.3, random_state=42)

X_train_chicago_encoded = pd.get_dummies(X_train_chicago)
X_test_chicago_encoded = pd.get_dummies(X_test_chicago)

X_train_ny_encoded = pd.get_dummies(X_train_ny)
X_test_ny_encoded = pd.get_dummies(X_test_ny)

X_test_chicago_encoded_aligned = drop_missing_columns(X_test_chicago_encoded).reindex(columns=X_train_chicago_encoded.columns, fill_value=0)
X_test_ny_encoded_aligned = drop_missing_columns(X_test_ny_encoded).reindex(columns=X_train_ny_encoded.columns, fill_value=0)

rf_regressor_chicago = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor_chicago.fit(X_train_chicago_encoded, y_train_chicago)
rf_pred_chicago = rf_regressor_chicago.predict(X_test_chicago_encoded_aligned)

rf_regressor_ny = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor_ny.fit(X_train_ny_encoded, y_train_ny)
rf_pred_ny = rf_regressor_ny.predict(X_test_ny_encoded_aligned)

xgb_regressor_chicago = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.8, learning_rate=0.05,
                                          max_depth=8, alpha=5, n_estimators=200)
xgb_regressor_chicago.fit(X_train_chicago_encoded, y_train_chicago)
xgb_pred_chicago = xgb_regressor_chicago.predict(X_test_chicago_encoded_aligned)

xgb_regressor_ny = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.8, learning_rate=0.05,
                                     max_depth=8, alpha=5, n_estimators=200)
xgb_regressor_ny.fit(X_train_ny_encoded, y_train_ny)
xgb_pred_ny = xgb_regressor_ny.predict(X_test_ny_encoded_aligned)

print("Random Forest Regressor for Chicago:")
print("Mean Squared Error:", mean_squared_error(y_test_chicago, rf_pred_chicago))
print("R-squared:", r2_score(y_test_chicago, rf_pred_chicago))

print("\nRandom Forest Regressor for New York:")
print("Mean Squared Error:", mean_squared_error(y_test_ny, rf_pred_ny))
print("R-squared:", r2_score(y_test_ny, rf_pred_ny))

print("\nXGBoost Regressor for Chicago:")
print("Mean Squared Error:", mean_squared_error(y_test_chicago, xgb_pred_chicago))
print("R-squared:", r2_score(y_test_chicago, xgb_pred_chicago))

print("\nXGBoost Regressor for New York:")
print("Mean Squared Error:", mean_squared_error(y_test_ny, xgb_pred_ny))
print("R-squared:", r2_score(y_test_ny, xgb_pred_ny))

rf_residuals_chicago = y_test_chicago - rf_pred_chicago
rf_residuals_ny = y_test_ny - rf_pred_ny

xgb_residuals_chicago = y_test_chicago - xgb_pred_chicago
xgb_residuals_ny = y_test_ny - xgb_pred_ny

plt.figure(figsize=(12, 6))
plt.subplot(2, 2, 1)
plt.hist(rf_residuals_chicago, bins=30, color='blue', alpha=0.7)
plt.title('Random Forest Residuals - Chicago')
plt.xlabel('Residuals')
plt.ylabel('Frequency')

plt.subplot(2, 2, 2)
plt.hist(rf_residuals_ny, bins=30, color='red', alpha=0.7)
plt.title('Random Forest Residuals - New York')
plt.xlabel('Residuals')
plt.ylabel('Frequency')

plt.subplot(2, 2, 3)
plt.hist(xgb_residuals_chicago, bins=30, color='blue', alpha=0.7)
plt.title('XGBoost Residuals - Chicago')
plt.xlabel('Residuals')
plt.ylabel('Frequency')

plt.subplot(2, 2, 4)
plt.hist(xgb_residuals_ny, bins=30, color='red', alpha=0.7)
plt.title('XGBoost Residuals - New York')
plt.xlabel('Residuals')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""## Model 3: ARIMA Model"""

Chicago_df.columns

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf
from sklearn.linear_model import LinearRegression

selected_crime_types = ['HOMICIDE', 'ASSAULT', 'ROBBERY', 'ARSON']
NY_df_selected = NY_df[NY_df['OFNS_DESC'].isin(selected_crime_types)]
Chicago_df_selected = Chicago_df[Chicago_df['Primary Type'].isin(selected_crime_types)]

plt.figure(figsize=(10, 6))
sns.countplot(x='OFNS_DESC', data=NY_df_selected)
plt.title('Number of Crimes by Type in New York')
plt.xlabel('Crime Type')
plt.ylabel('Count')
plt.show()

# Plotting crimes on time horizon by type (for Chicago)
plt.figure(figsize=(10, 6))
sns.countplot(x='Primary Type', data=Chicago_df_selected)
plt.title('Number of Crimes by Type in Chicago')
plt.xlabel('Crime Type')
plt.ylabel('Count')
plt.show()

NY_df_time_series = NY_df_selected[['ARREST_DATE', 'OFNS_DESC']]
NY_df_time_series['ARREST_DATE'] = pd.to_datetime(NY_df_time_series['ARREST_DATE'])
NY_df_time_series.set_index('ARREST_DATE', inplace=True)
NY_df_time_series_grouped = NY_df_time_series.groupby([pd.Grouper(freq='M'), 'OFNS_DESC']).size().unstack(fill_value=0)

NY_df_time_series_grouped.plot(figsize=(10, 6))
plt.xlabel('Year')
plt.ylabel('Number of Arrests')
plt.title('Arrest Trends Over Time for Different Crime Types in New York')
plt.show()

correlation_matrix = NY_df_time_series_grouped.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix for Crime Types in New York')
plt.show()

Chicago_df_time_series = Chicago_df[['Date', 'Primary Type']]
Chicago_df_time_series['Date'] = pd.to_datetime(Chicago_df_time_series['Date'])
Chicago_df_time_series.set_index('Date', inplace=True)
Chicago_df_time_series_grouped = Chicago_df_time_series.groupby([pd.Grouper(freq='M'), 'Primary Type']).size().unstack(fill_value=0)

Chicago_df_time_series_grouped.plot(figsize=(10, 6))
plt.title('Monthly Crime Counts in Chicago by Type')
plt.xlabel('Date')
plt.ylabel('Number of Crimes')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

def plot_faceted_monthly_crime_counts(data):
    """
    Plot monthly crime counts in Chicago by type using a faceted plot.

    """

    melted_data = data.reset_index().melt(id_vars='Date', var_name='Crime Type', value_name='Count')


    g = sns.relplot(data=melted_data, x='Date', y='Count', col='Crime Type', col_wrap=3,
                    kind='line', aspect=1.5, height=3, marker='o')
    g.fig.suptitle('Monthly Crime Counts in Chicago by Type', y=1.05)
    g.set_axis_labels('Date', 'Number of Crimes')
    g.tight_layout()
    plt.show()


plot_faceted_monthly_crime_counts(Chicago_df_time_series_grouped)

import plotly.express as px

def plot_interactive_geographic_analysis(data):
    """
    Plot crime incidents in Chicago by type using an interactive plot.

    """
    # Aggregate less frequent categories
    top_crime_types = data['Primary Type'].value_counts().nlargest(5).index
    filtered_data = data[data['Primary Type'].isin(top_crime_types)]


    fig = px.scatter_mapbox(filtered_data, lat="Latitude", lon="Longitude", color="Primary Type",
                            hover_name="Primary Type", zoom=10, height=600)
    fig.update_layout(mapbox_style="carto-positron", title='Crime Incidents in Chicago by Type',
                      margin={"r":0,"t":30,"l":0,"b":0})
    fig.show()


plot_interactive_geographic_analysis(Chicago_df)

# Temporal Analysis
Chicago_df['Date'] = pd.to_datetime(Chicago_df['Date'])
Chicago_df['Month'] = Chicago_df['Date'].dt.month
monthly_crime_counts = Chicago_df.groupby(['Primary Type', 'Month']).size().unstack(fill_value=0)

plt.figure(figsize=(12, 8))
monthly_crime_counts.plot(kind='line')
plt.title('Monthly Crime Trends in Chicago by Type')
plt.xlabel('Month')
plt.ylabel('Number of Crimes')
plt.legend(title='Crime Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45, ha='right')
plt.show()

import plotly.express as px

def plot_interactive_geographic_analysis_ny(data):

    # Aggregate less frequent categories
    top_crime_types = data['OFNS_DESC'].value_counts().nlargest(5).index
    filtered_data = data[data['OFNS_DESC'].isin(top_crime_types)]

    # Plot using Plotly express
    fig = px.scatter_mapbox(filtered_data, lat="Latitude", lon="Longitude", color="OFNS_DESC",
                            hover_name="OFNS_DESC", zoom=10, height=600)
    fig.update_layout(mapbox_style="carto-positron", title='Crime Incidents in New York by Type',
                      margin={"r":0,"t":30,"l":0,"b":0})
    fig.show()

# Example usage
plot_interactive_geographic_analysis_ny(NY_df)

from statsmodels.tsa.arima.model import ARIMA

crime_types = Chicago_df_time_series_grouped.columns
arima_models = {}

for crime_type in crime_types:
    endog_variable = Chicago_df_time_series_grouped[crime_type]
    arima_model = ARIMA(endog_variable, order=(5, 1, 0))
    arima_model_fit = arima_model.fit()
    arima_models[crime_type] = arima_model_fit

for crime_type, model in arima_models.items():
    print(f"ARIMA Model Summary for {crime_type}:")
    print(model.summary())
    print("\n")

from statsmodels.tsa.arima.model import ARIMA

crime_type = 'ROBBERY'
ny_crime_series = NY_df_time_series_grouped[crime_type]

ny_model = ARIMA(ny_crime_series, order=(5, 1, 0))
ny_model_fit = ny_model.fit()

print(ny_model_fit.summary())

